{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Credit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('Bases de dados/credit.pkl', 'rb') as f:\n",
    "    X_credit_treinamento, y_credit_treinamento, X_credit_teste, y_credit_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 3), (1500,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_credit_treinamento.shape, y_credit_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 3), (500,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_credit_teste.shape, y_credit_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3 + 1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.99590574\n",
      "Iteration 2, loss = 0.98287921\n",
      "Iteration 3, loss = 0.96986059\n",
      "Iteration 4, loss = 0.95706868\n",
      "Iteration 5, loss = 0.94411686\n",
      "Iteration 6, loss = 0.93135298\n",
      "Iteration 7, loss = 0.91853757\n",
      "Iteration 8, loss = 0.90553964\n",
      "Iteration 9, loss = 0.89258915\n",
      "Iteration 10, loss = 0.87952104\n",
      "Iteration 11, loss = 0.86649390\n",
      "Iteration 12, loss = 0.85321325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.83988555\n",
      "Iteration 14, loss = 0.82632859\n",
      "Iteration 15, loss = 0.81272395\n",
      "Iteration 16, loss = 0.79893042\n",
      "Iteration 17, loss = 0.78500200\n",
      "Iteration 18, loss = 0.77099548\n",
      "Iteration 19, loss = 0.75689699\n",
      "Iteration 20, loss = 0.74270978\n",
      "Iteration 21, loss = 0.72837993\n",
      "Iteration 22, loss = 0.71413652\n",
      "Iteration 23, loss = 0.69970027\n",
      "Iteration 24, loss = 0.68540917\n",
      "Iteration 25, loss = 0.67096135\n",
      "Iteration 26, loss = 0.65680550\n",
      "Iteration 27, loss = 0.64247043\n",
      "Iteration 28, loss = 0.62827519\n",
      "Iteration 29, loss = 0.61427757\n",
      "Iteration 30, loss = 0.60043554\n",
      "Iteration 31, loss = 0.58661492\n",
      "Iteration 32, loss = 0.57324456\n",
      "Iteration 33, loss = 0.56000995\n",
      "Iteration 34, loss = 0.54693547\n",
      "Iteration 35, loss = 0.53427378\n",
      "Iteration 36, loss = 0.52191108\n",
      "Iteration 37, loss = 0.50979405\n",
      "Iteration 38, loss = 0.49816986\n",
      "Iteration 39, loss = 0.48676880\n",
      "Iteration 40, loss = 0.47583471\n",
      "Iteration 41, loss = 0.46519122\n",
      "Iteration 42, loss = 0.45492653\n",
      "Iteration 43, loss = 0.44509661\n",
      "Iteration 44, loss = 0.43561506\n",
      "Iteration 45, loss = 0.42658592\n",
      "Iteration 46, loss = 0.41776657\n",
      "Iteration 47, loss = 0.40944361\n",
      "Iteration 48, loss = 0.40145238\n",
      "Iteration 49, loss = 0.39388174\n",
      "Iteration 50, loss = 0.38669130\n",
      "Iteration 51, loss = 0.37979700\n",
      "Iteration 52, loss = 0.37313840\n",
      "Iteration 53, loss = 0.36694118\n",
      "Iteration 54, loss = 0.36107272\n",
      "Iteration 55, loss = 0.35538410\n",
      "Iteration 56, loss = 0.35000830\n",
      "Iteration 57, loss = 0.34487598\n",
      "Iteration 58, loss = 0.33998031\n",
      "Iteration 59, loss = 0.33540698\n",
      "Iteration 60, loss = 0.33095019\n",
      "Iteration 61, loss = 0.32669690\n",
      "Iteration 62, loss = 0.32272276\n",
      "Iteration 63, loss = 0.31884945\n",
      "Iteration 64, loss = 0.31522887\n",
      "Iteration 65, loss = 0.31167351\n",
      "Iteration 66, loss = 0.30831866\n",
      "Iteration 67, loss = 0.30508554\n",
      "Iteration 68, loss = 0.30195063\n",
      "Iteration 69, loss = 0.29899807\n",
      "Iteration 70, loss = 0.29614447\n",
      "Iteration 71, loss = 0.29337267\n",
      "Iteration 72, loss = 0.29072985\n",
      "Iteration 73, loss = 0.28819104\n",
      "Iteration 74, loss = 0.28570459\n",
      "Iteration 75, loss = 0.28336098\n",
      "Iteration 76, loss = 0.28107569\n",
      "Iteration 77, loss = 0.27888243\n",
      "Iteration 78, loss = 0.27676991\n",
      "Iteration 79, loss = 0.27474386\n",
      "Iteration 80, loss = 0.27273952\n",
      "Iteration 81, loss = 0.27086384\n",
      "Iteration 82, loss = 0.26900465\n",
      "Iteration 83, loss = 0.26718075\n",
      "Iteration 84, loss = 0.26546733\n",
      "Iteration 85, loss = 0.26375185\n",
      "Iteration 86, loss = 0.26209762\n",
      "Iteration 87, loss = 0.26046307\n",
      "Iteration 88, loss = 0.25890392\n",
      "Iteration 89, loss = 0.25733451\n",
      "Iteration 90, loss = 0.25584030\n",
      "Iteration 91, loss = 0.25436321\n",
      "Iteration 92, loss = 0.25292193\n",
      "Iteration 93, loss = 0.25152129\n",
      "Iteration 94, loss = 0.25016556\n",
      "Iteration 95, loss = 0.24882991\n",
      "Iteration 96, loss = 0.24750421\n",
      "Iteration 97, loss = 0.24622333\n",
      "Iteration 98, loss = 0.24495661\n",
      "Iteration 99, loss = 0.24369946\n",
      "Iteration 100, loss = 0.24247478\n",
      "Iteration 101, loss = 0.24127949\n",
      "Iteration 102, loss = 0.24008992\n",
      "Iteration 103, loss = 0.23890215\n",
      "Iteration 104, loss = 0.23777533\n",
      "Iteration 105, loss = 0.23661764\n",
      "Iteration 106, loss = 0.23551591\n",
      "Iteration 107, loss = 0.23441897\n",
      "Iteration 108, loss = 0.23332589\n",
      "Iteration 109, loss = 0.23224441\n",
      "Iteration 110, loss = 0.23116279\n",
      "Iteration 111, loss = 0.23014033\n",
      "Iteration 112, loss = 0.22907960\n",
      "Iteration 113, loss = 0.22803312\n",
      "Iteration 114, loss = 0.22699663\n",
      "Iteration 115, loss = 0.22598465\n",
      "Iteration 116, loss = 0.22498653\n",
      "Iteration 117, loss = 0.22397631\n",
      "Iteration 118, loss = 0.22298409\n",
      "Iteration 119, loss = 0.22199363\n",
      "Iteration 120, loss = 0.22101110\n",
      "Iteration 121, loss = 0.22001908\n",
      "Iteration 122, loss = 0.21905483\n",
      "Iteration 123, loss = 0.21807014\n",
      "Iteration 124, loss = 0.21711020\n",
      "Iteration 125, loss = 0.21615499\n",
      "Iteration 126, loss = 0.21521027\n",
      "Iteration 127, loss = 0.21423703\n",
      "Iteration 128, loss = 0.21328396\n",
      "Iteration 129, loss = 0.21232218\n",
      "Iteration 130, loss = 0.21137299\n",
      "Iteration 131, loss = 0.21042936\n",
      "Iteration 132, loss = 0.20946759\n",
      "Iteration 133, loss = 0.20854206\n",
      "Iteration 134, loss = 0.20757346\n",
      "Iteration 135, loss = 0.20658477\n",
      "Iteration 136, loss = 0.20563032\n",
      "Iteration 137, loss = 0.20465096\n",
      "Iteration 138, loss = 0.20365470\n",
      "Iteration 139, loss = 0.20266302\n",
      "Iteration 140, loss = 0.20168545\n",
      "Iteration 141, loss = 0.20064641\n",
      "Iteration 142, loss = 0.19965004\n",
      "Iteration 143, loss = 0.19863998\n",
      "Iteration 144, loss = 0.19763067\n",
      "Iteration 145, loss = 0.19657186\n",
      "Iteration 146, loss = 0.19556077\n",
      "Iteration 147, loss = 0.19451514\n",
      "Iteration 148, loss = 0.19349061\n",
      "Iteration 149, loss = 0.19242953\n",
      "Iteration 150, loss = 0.19140901\n",
      "Iteration 151, loss = 0.19035486\n",
      "Iteration 152, loss = 0.18927949\n",
      "Iteration 153, loss = 0.18825570\n",
      "Iteration 154, loss = 0.18716488\n",
      "Iteration 155, loss = 0.18606492\n",
      "Iteration 156, loss = 0.18494249\n",
      "Iteration 157, loss = 0.18379383\n",
      "Iteration 158, loss = 0.18265880\n",
      "Iteration 159, loss = 0.18148243\n",
      "Iteration 160, loss = 0.18030008\n",
      "Iteration 161, loss = 0.17913630\n",
      "Iteration 162, loss = 0.17795178\n",
      "Iteration 163, loss = 0.17674920\n",
      "Iteration 164, loss = 0.17553520\n",
      "Iteration 165, loss = 0.17432448\n",
      "Iteration 166, loss = 0.17307667\n",
      "Iteration 167, loss = 0.17187462\n",
      "Iteration 168, loss = 0.17065909\n",
      "Iteration 169, loss = 0.16942529\n",
      "Iteration 170, loss = 0.16822138\n",
      "Iteration 171, loss = 0.16698043\n",
      "Iteration 172, loss = 0.16575429\n",
      "Iteration 173, loss = 0.16451707\n",
      "Iteration 174, loss = 0.16327364\n",
      "Iteration 175, loss = 0.16204562\n",
      "Iteration 176, loss = 0.16076963\n",
      "Iteration 177, loss = 0.15950016\n",
      "Iteration 178, loss = 0.15818113\n",
      "Iteration 179, loss = 0.15690432\n",
      "Iteration 180, loss = 0.15560131\n",
      "Iteration 181, loss = 0.15428279\n",
      "Iteration 182, loss = 0.15298463\n",
      "Iteration 183, loss = 0.15163263\n",
      "Iteration 184, loss = 0.15035526\n",
      "Iteration 185, loss = 0.14907748\n",
      "Iteration 186, loss = 0.14774074\n",
      "Iteration 187, loss = 0.14651237\n",
      "Iteration 188, loss = 0.14519318\n",
      "Iteration 189, loss = 0.14389253\n",
      "Iteration 190, loss = 0.14263751\n",
      "Iteration 191, loss = 0.14135248\n",
      "Iteration 192, loss = 0.14013541\n",
      "Iteration 193, loss = 0.13892090\n",
      "Iteration 194, loss = 0.13769218\n",
      "Iteration 195, loss = 0.13644621\n",
      "Iteration 196, loss = 0.13517789\n",
      "Iteration 197, loss = 0.13394936\n",
      "Iteration 198, loss = 0.13273594\n",
      "Iteration 199, loss = 0.13150212\n",
      "Iteration 200, loss = 0.13024232\n",
      "Iteration 201, loss = 0.12901522\n",
      "Iteration 202, loss = 0.12778311\n",
      "Iteration 203, loss = 0.12656459\n",
      "Iteration 204, loss = 0.12543396\n",
      "Iteration 205, loss = 0.12423459\n",
      "Iteration 206, loss = 0.12312858\n",
      "Iteration 207, loss = 0.12199303\n",
      "Iteration 208, loss = 0.12088221\n",
      "Iteration 209, loss = 0.11977685\n",
      "Iteration 210, loss = 0.11869876\n",
      "Iteration 211, loss = 0.11767690\n",
      "Iteration 212, loss = 0.11667531\n",
      "Iteration 213, loss = 0.11563410\n",
      "Iteration 214, loss = 0.11460537\n",
      "Iteration 215, loss = 0.11365462\n",
      "Iteration 216, loss = 0.11265861\n",
      "Iteration 217, loss = 0.11168963\n",
      "Iteration 218, loss = 0.11073220\n",
      "Iteration 219, loss = 0.10975504\n",
      "Iteration 220, loss = 0.10883257\n",
      "Iteration 221, loss = 0.10786620\n",
      "Iteration 222, loss = 0.10693911\n",
      "Iteration 223, loss = 0.10603692\n",
      "Iteration 224, loss = 0.10515663\n",
      "Iteration 225, loss = 0.10426262\n",
      "Iteration 226, loss = 0.10342319\n",
      "Iteration 227, loss = 0.10255093\n",
      "Iteration 228, loss = 0.10171361\n",
      "Iteration 229, loss = 0.10090719\n",
      "Iteration 230, loss = 0.10006321\n",
      "Iteration 231, loss = 0.09928731\n",
      "Iteration 232, loss = 0.09848386\n",
      "Iteration 233, loss = 0.09769206\n",
      "Iteration 234, loss = 0.09693211\n",
      "Iteration 235, loss = 0.09616936\n",
      "Iteration 236, loss = 0.09542085\n",
      "Iteration 237, loss = 0.09468449\n",
      "Iteration 238, loss = 0.09394682\n",
      "Iteration 239, loss = 0.09320954\n",
      "Iteration 240, loss = 0.09250293\n",
      "Iteration 241, loss = 0.09178223\n",
      "Iteration 242, loss = 0.09110728\n",
      "Iteration 243, loss = 0.09040181\n",
      "Iteration 244, loss = 0.08972975\n",
      "Iteration 245, loss = 0.08905845\n",
      "Iteration 246, loss = 0.08839743\n",
      "Iteration 247, loss = 0.08771232\n",
      "Iteration 248, loss = 0.08709863\n",
      "Iteration 249, loss = 0.08643191\n",
      "Iteration 250, loss = 0.08582759\n",
      "Iteration 251, loss = 0.08518133\n",
      "Iteration 252, loss = 0.08454956\n",
      "Iteration 253, loss = 0.08393012\n",
      "Iteration 254, loss = 0.08331402\n",
      "Iteration 255, loss = 0.08269917\n",
      "Iteration 256, loss = 0.08209081\n",
      "Iteration 257, loss = 0.08150286\n",
      "Iteration 258, loss = 0.08091524\n",
      "Iteration 259, loss = 0.08033511\n",
      "Iteration 260, loss = 0.07972523\n",
      "Iteration 261, loss = 0.07916930\n",
      "Iteration 262, loss = 0.07861873\n",
      "Iteration 263, loss = 0.07801145\n",
      "Iteration 264, loss = 0.07747499\n",
      "Iteration 265, loss = 0.07693649\n",
      "Iteration 266, loss = 0.07637625\n",
      "Iteration 267, loss = 0.07581883\n",
      "Iteration 268, loss = 0.07527693\n",
      "Iteration 269, loss = 0.07475375\n",
      "Iteration 270, loss = 0.07423262\n",
      "Iteration 271, loss = 0.07373168\n",
      "Iteration 272, loss = 0.07320134\n",
      "Iteration 273, loss = 0.07267973\n",
      "Iteration 274, loss = 0.07219205\n",
      "Iteration 275, loss = 0.07169028\n",
      "Iteration 276, loss = 0.07120148\n",
      "Iteration 277, loss = 0.07074099\n",
      "Iteration 278, loss = 0.07022303\n",
      "Iteration 279, loss = 0.06974972\n",
      "Iteration 280, loss = 0.06929531\n",
      "Iteration 281, loss = 0.06885240\n",
      "Iteration 282, loss = 0.06836001\n",
      "Iteration 283, loss = 0.06790338\n",
      "Iteration 284, loss = 0.06744885\n",
      "Iteration 285, loss = 0.06701267\n",
      "Iteration 286, loss = 0.06656789\n",
      "Iteration 287, loss = 0.06610888\n",
      "Iteration 288, loss = 0.06569524\n",
      "Iteration 289, loss = 0.06527352\n",
      "Iteration 290, loss = 0.06479254\n",
      "Iteration 291, loss = 0.06443297\n",
      "Iteration 292, loss = 0.06391978\n",
      "Iteration 293, loss = 0.06349394\n",
      "Iteration 294, loss = 0.06305652\n",
      "Iteration 295, loss = 0.06266658\n",
      "Iteration 296, loss = 0.06221547\n",
      "Iteration 297, loss = 0.06179145\n",
      "Iteration 298, loss = 0.06137212\n",
      "Iteration 299, loss = 0.06095216\n",
      "Iteration 300, loss = 0.06054035\n",
      "Iteration 301, loss = 0.06011700\n",
      "Iteration 302, loss = 0.05971907\n",
      "Iteration 303, loss = 0.05932553\n",
      "Iteration 304, loss = 0.05889525\n",
      "Iteration 305, loss = 0.05850138\n",
      "Iteration 306, loss = 0.05813597\n",
      "Iteration 307, loss = 0.05773385\n",
      "Iteration 308, loss = 0.05737534\n",
      "Iteration 309, loss = 0.05699642\n",
      "Iteration 310, loss = 0.05662980\n",
      "Iteration 311, loss = 0.05626107\n",
      "Iteration 312, loss = 0.05587596\n",
      "Iteration 313, loss = 0.05552886\n",
      "Iteration 314, loss = 0.05515314\n",
      "Iteration 315, loss = 0.05480947\n",
      "Iteration 316, loss = 0.05443608\n",
      "Iteration 317, loss = 0.05409170\n",
      "Iteration 318, loss = 0.05374028\n",
      "Iteration 319, loss = 0.05339601\n",
      "Iteration 320, loss = 0.05306325\n",
      "Iteration 321, loss = 0.05271074\n",
      "Iteration 322, loss = 0.05237081\n",
      "Iteration 323, loss = 0.05202954\n",
      "Iteration 324, loss = 0.05170722\n",
      "Iteration 325, loss = 0.05135789\n",
      "Iteration 326, loss = 0.05104838\n",
      "Iteration 327, loss = 0.05072866\n",
      "Iteration 328, loss = 0.05037732\n",
      "Iteration 329, loss = 0.05006968\n",
      "Iteration 330, loss = 0.04973585\n",
      "Iteration 331, loss = 0.04942134\n",
      "Iteration 332, loss = 0.04912357\n",
      "Iteration 333, loss = 0.04880717\n",
      "Iteration 334, loss = 0.04848425\n",
      "Iteration 335, loss = 0.04818611\n",
      "Iteration 336, loss = 0.04787573\n",
      "Iteration 337, loss = 0.04757287\n",
      "Iteration 338, loss = 0.04727796\n",
      "Iteration 339, loss = 0.04696551\n",
      "Iteration 340, loss = 0.04671369\n",
      "Iteration 341, loss = 0.04643599\n",
      "Iteration 342, loss = 0.04612753\n",
      "Iteration 343, loss = 0.04584927\n",
      "Iteration 344, loss = 0.04554344\n",
      "Iteration 345, loss = 0.04526942\n",
      "Iteration 346, loss = 0.04500896\n",
      "Iteration 347, loss = 0.04474649\n",
      "Iteration 348, loss = 0.04448043\n",
      "Iteration 349, loss = 0.04420733\n",
      "Iteration 350, loss = 0.04396435\n",
      "Iteration 351, loss = 0.04370060\n",
      "Iteration 352, loss = 0.04343512\n",
      "Iteration 353, loss = 0.04319472\n",
      "Iteration 354, loss = 0.04293561\n",
      "Iteration 355, loss = 0.04268709\n",
      "Iteration 356, loss = 0.04245189\n",
      "Iteration 357, loss = 0.04220091\n",
      "Iteration 358, loss = 0.04194765\n",
      "Iteration 359, loss = 0.04171956\n",
      "Iteration 360, loss = 0.04147576\n",
      "Iteration 361, loss = 0.04124269\n",
      "Iteration 362, loss = 0.04098475\n",
      "Iteration 363, loss = 0.04076167\n",
      "Iteration 364, loss = 0.04055909\n",
      "Iteration 365, loss = 0.04031616\n",
      "Iteration 366, loss = 0.04008912\n",
      "Iteration 367, loss = 0.03984302\n",
      "Iteration 368, loss = 0.03966252\n",
      "Iteration 369, loss = 0.03940646\n",
      "Iteration 370, loss = 0.03919204\n",
      "Iteration 371, loss = 0.03896884\n",
      "Iteration 372, loss = 0.03875158\n",
      "Iteration 373, loss = 0.03853303\n",
      "Iteration 374, loss = 0.03833362\n",
      "Iteration 375, loss = 0.03812258\n",
      "Iteration 376, loss = 0.03794630\n",
      "Iteration 377, loss = 0.03771047\n",
      "Iteration 378, loss = 0.03753560\n",
      "Iteration 379, loss = 0.03731617\n",
      "Iteration 380, loss = 0.03709920\n",
      "Iteration 381, loss = 0.03693368\n",
      "Iteration 382, loss = 0.03672449\n",
      "Iteration 383, loss = 0.03653185\n",
      "Iteration 384, loss = 0.03634668\n",
      "Iteration 385, loss = 0.03615375\n",
      "Iteration 386, loss = 0.03597973\n",
      "Iteration 387, loss = 0.03578758\n",
      "Iteration 388, loss = 0.03561446\n",
      "Iteration 389, loss = 0.03542242\n",
      "Iteration 390, loss = 0.03520769\n",
      "Iteration 391, loss = 0.03506591\n",
      "Iteration 392, loss = 0.03486499\n",
      "Iteration 393, loss = 0.03471644\n",
      "Iteration 394, loss = 0.03451530\n",
      "Iteration 395, loss = 0.03433654\n",
      "Iteration 396, loss = 0.03416686\n",
      "Iteration 397, loss = 0.03403480\n",
      "Iteration 398, loss = 0.03384428\n",
      "Iteration 399, loss = 0.03366193\n",
      "Iteration 400, loss = 0.03350521\n",
      "Iteration 401, loss = 0.03331586\n",
      "Iteration 402, loss = 0.03317859\n",
      "Iteration 403, loss = 0.03302279\n",
      "Iteration 404, loss = 0.03283735\n",
      "Iteration 405, loss = 0.03268989\n",
      "Iteration 406, loss = 0.03250471\n",
      "Iteration 407, loss = 0.03236282\n",
      "Iteration 408, loss = 0.03222331\n",
      "Iteration 409, loss = 0.03204309\n",
      "Iteration 410, loss = 0.03190553\n",
      "Iteration 411, loss = 0.03173306\n",
      "Iteration 412, loss = 0.03160278\n",
      "Iteration 413, loss = 0.03143116\n",
      "Iteration 414, loss = 0.03130308\n",
      "Iteration 415, loss = 0.03113888\n",
      "Iteration 416, loss = 0.03100734\n",
      "Iteration 417, loss = 0.03085423\n",
      "Iteration 418, loss = 0.03071869\n",
      "Iteration 419, loss = 0.03059746\n",
      "Iteration 420, loss = 0.03044868\n",
      "Iteration 421, loss = 0.03030354\n",
      "Iteration 422, loss = 0.03017393\n",
      "Iteration 423, loss = 0.03003027\n",
      "Iteration 424, loss = 0.02987408\n",
      "Iteration 425, loss = 0.02978307\n",
      "Iteration 426, loss = 0.02963501\n",
      "Iteration 427, loss = 0.02947235\n",
      "Iteration 428, loss = 0.02934453\n",
      "Iteration 429, loss = 0.02923212\n",
      "Iteration 430, loss = 0.02911758\n",
      "Iteration 431, loss = 0.02894920\n",
      "Iteration 432, loss = 0.02888611\n",
      "Iteration 433, loss = 0.02871050\n",
      "Iteration 434, loss = 0.02860168\n",
      "Iteration 435, loss = 0.02845136\n",
      "Iteration 436, loss = 0.02833466\n",
      "Iteration 437, loss = 0.02821495\n",
      "Iteration 438, loss = 0.02810098\n",
      "Iteration 439, loss = 0.02796819\n",
      "Iteration 440, loss = 0.02786561\n",
      "Iteration 441, loss = 0.02772647\n",
      "Iteration 442, loss = 0.02761946\n",
      "Iteration 443, loss = 0.02748240\n",
      "Iteration 444, loss = 0.02737814\n",
      "Iteration 445, loss = 0.02726526\n",
      "Iteration 446, loss = 0.02717369\n",
      "Iteration 447, loss = 0.02701176\n",
      "Iteration 448, loss = 0.02689467\n",
      "Iteration 449, loss = 0.02679955\n",
      "Iteration 450, loss = 0.02666934\n",
      "Iteration 451, loss = 0.02654727\n",
      "Iteration 452, loss = 0.02642751\n",
      "Iteration 453, loss = 0.02636120\n",
      "Iteration 454, loss = 0.02623541\n",
      "Iteration 455, loss = 0.02607821\n",
      "Iteration 456, loss = 0.02598215\n",
      "Iteration 457, loss = 0.02585985\n",
      "Iteration 458, loss = 0.02574247\n",
      "Iteration 459, loss = 0.02563354\n",
      "Iteration 460, loss = 0.02553338\n",
      "Iteration 461, loss = 0.02542870\n",
      "Iteration 462, loss = 0.02532807\n",
      "Iteration 463, loss = 0.02522967\n",
      "Iteration 464, loss = 0.02509518\n",
      "Iteration 465, loss = 0.02502150\n",
      "Iteration 466, loss = 0.02491321\n",
      "Iteration 467, loss = 0.02479305\n",
      "Iteration 468, loss = 0.02469160\n",
      "Iteration 469, loss = 0.02458132\n",
      "Iteration 470, loss = 0.02449053\n",
      "Iteration 471, loss = 0.02436759\n",
      "Iteration 472, loss = 0.02430576\n",
      "Iteration 473, loss = 0.02422070\n",
      "Iteration 474, loss = 0.02407116\n",
      "Iteration 475, loss = 0.02397684\n",
      "Iteration 476, loss = 0.02388207\n",
      "Iteration 477, loss = 0.02378898\n",
      "Iteration 478, loss = 0.02367630\n",
      "Iteration 479, loss = 0.02362065\n",
      "Iteration 480, loss = 0.02351308\n",
      "Iteration 481, loss = 0.02339879\n",
      "Iteration 482, loss = 0.02330280\n",
      "Iteration 483, loss = 0.02320566\n",
      "Iteration 484, loss = 0.02311413\n",
      "Iteration 485, loss = 0.02304767\n",
      "Iteration 486, loss = 0.02292653\n",
      "Iteration 487, loss = 0.02284994\n",
      "Iteration 488, loss = 0.02277263\n",
      "Iteration 489, loss = 0.02270832\n",
      "Iteration 490, loss = 0.02258561\n",
      "Iteration 491, loss = 0.02251347\n",
      "Iteration 492, loss = 0.02240200\n",
      "Iteration 493, loss = 0.02232954\n",
      "Iteration 494, loss = 0.02222114\n",
      "Iteration 495, loss = 0.02214055\n",
      "Iteration 496, loss = 0.02205555\n",
      "Iteration 497, loss = 0.02199874\n",
      "Iteration 498, loss = 0.02187506\n",
      "Iteration 499, loss = 0.02177391\n",
      "Iteration 500, loss = 0.02170699\n",
      "Iteration 501, loss = 0.02165878\n",
      "Iteration 502, loss = 0.02155165\n",
      "Iteration 503, loss = 0.02147504\n",
      "Iteration 504, loss = 0.02138733\n",
      "Iteration 505, loss = 0.02130400\n",
      "Iteration 506, loss = 0.02123016\n",
      "Iteration 507, loss = 0.02112889\n",
      "Iteration 508, loss = 0.02105260\n",
      "Iteration 509, loss = 0.02098221\n",
      "Iteration 510, loss = 0.02091705\n",
      "Iteration 511, loss = 0.02081272\n",
      "Iteration 512, loss = 0.02075147\n",
      "Iteration 513, loss = 0.02066699\n",
      "Iteration 514, loss = 0.02058135\n",
      "Iteration 515, loss = 0.02050496\n",
      "Iteration 516, loss = 0.02042102\n",
      "Iteration 517, loss = 0.02036030\n",
      "Iteration 518, loss = 0.02026934\n",
      "Iteration 519, loss = 0.02020976\n",
      "Iteration 520, loss = 0.02013366\n",
      "Iteration 521, loss = 0.02005368\n",
      "Iteration 522, loss = 0.01997913\n",
      "Iteration 523, loss = 0.01992795\n",
      "Iteration 524, loss = 0.01984144\n",
      "Iteration 525, loss = 0.01978303\n",
      "Iteration 526, loss = 0.01970208\n",
      "Iteration 527, loss = 0.01961703\n",
      "Iteration 528, loss = 0.01954860\n",
      "Iteration 529, loss = 0.01947404\n",
      "Iteration 530, loss = 0.01940861\n",
      "Iteration 531, loss = 0.01933313\n",
      "Iteration 532, loss = 0.01925881\n",
      "Iteration 533, loss = 0.01919464\n",
      "Iteration 534, loss = 0.01921024\n",
      "Iteration 535, loss = 0.01905155\n",
      "Iteration 536, loss = 0.01899052\n",
      "Iteration 537, loss = 0.01892788\n",
      "Iteration 538, loss = 0.01886856\n",
      "Iteration 539, loss = 0.01877441\n",
      "Iteration 540, loss = 0.01872142\n",
      "Iteration 541, loss = 0.01864811\n",
      "Iteration 542, loss = 0.01857852\n",
      "Iteration 543, loss = 0.01849931\n",
      "Iteration 544, loss = 0.01845810\n",
      "Iteration 545, loss = 0.01836231\n",
      "Iteration 546, loss = 0.01829710\n",
      "Iteration 547, loss = 0.01827072\n",
      "Iteration 548, loss = 0.01818568\n",
      "Iteration 549, loss = 0.01809959\n",
      "Iteration 550, loss = 0.01809181\n",
      "Iteration 551, loss = 0.01798869\n",
      "Iteration 552, loss = 0.01793465\n",
      "Iteration 553, loss = 0.01785375\n",
      "Iteration 554, loss = 0.01780800\n",
      "Iteration 555, loss = 0.01774073\n",
      "Iteration 556, loss = 0.01765449\n",
      "Iteration 557, loss = 0.01759942\n",
      "Iteration 558, loss = 0.01754117\n",
      "Iteration 559, loss = 0.01747393\n",
      "Iteration 560, loss = 0.01742092\n",
      "Iteration 561, loss = 0.01736665\n",
      "Iteration 562, loss = 0.01729137\n",
      "Iteration 563, loss = 0.01723643\n",
      "Iteration 564, loss = 0.01717796\n",
      "Iteration 565, loss = 0.01713028\n",
      "Iteration 566, loss = 0.01705289\n",
      "Iteration 567, loss = 0.01700033\n",
      "Iteration 568, loss = 0.01693862\n",
      "Iteration 569, loss = 0.01687534\n",
      "Iteration 570, loss = 0.01681985\n",
      "Iteration 571, loss = 0.01675553\n",
      "Iteration 572, loss = 0.01670247\n",
      "Iteration 573, loss = 0.01664406\n",
      "Iteration 574, loss = 0.01659215\n",
      "Iteration 575, loss = 0.01653251\n",
      "Iteration 576, loss = 0.01646375\n",
      "Iteration 577, loss = 0.01641533\n",
      "Iteration 578, loss = 0.01636308\n",
      "Iteration 579, loss = 0.01629815\n",
      "Iteration 580, loss = 0.01626598\n",
      "Iteration 581, loss = 0.01619122\n",
      "Iteration 582, loss = 0.01612860\n",
      "Iteration 583, loss = 0.01608749\n",
      "Iteration 584, loss = 0.01602996\n",
      "Iteration 585, loss = 0.01599132\n",
      "Iteration 586, loss = 0.01594662\n",
      "Iteration 587, loss = 0.01586947\n",
      "Iteration 588, loss = 0.01580854\n",
      "Iteration 589, loss = 0.01574223\n",
      "Iteration 590, loss = 0.01570715\n",
      "Iteration 591, loss = 0.01566115\n",
      "Iteration 592, loss = 0.01561708\n",
      "Iteration 593, loss = 0.01553591\n",
      "Iteration 594, loss = 0.01550102\n",
      "Iteration 595, loss = 0.01544749\n",
      "Iteration 596, loss = 0.01540470\n",
      "Iteration 597, loss = 0.01535367\n",
      "Iteration 598, loss = 0.01532122\n",
      "Iteration 599, loss = 0.01525009\n",
      "Iteration 600, loss = 0.01518931\n",
      "Iteration 601, loss = 0.01515426\n",
      "Iteration 602, loss = 0.01510736\n",
      "Iteration 603, loss = 0.01505700\n",
      "Iteration 604, loss = 0.01501364\n",
      "Iteration 605, loss = 0.01499538\n",
      "Iteration 606, loss = 0.01488768\n",
      "Iteration 607, loss = 0.01485877\n",
      "Iteration 608, loss = 0.01480507\n",
      "Iteration 609, loss = 0.01476167\n",
      "Iteration 610, loss = 0.01470166\n",
      "Iteration 611, loss = 0.01465795\n",
      "Iteration 612, loss = 0.01461367\n",
      "Iteration 613, loss = 0.01456918\n",
      "Iteration 614, loss = 0.01452750\n",
      "Iteration 615, loss = 0.01448662\n",
      "Iteration 616, loss = 0.01444318\n",
      "Iteration 617, loss = 0.01436721\n",
      "Iteration 618, loss = 0.01432304\n",
      "Iteration 619, loss = 0.01429944\n",
      "Iteration 620, loss = 0.01424826\n",
      "Iteration 621, loss = 0.01418567\n",
      "Iteration 622, loss = 0.01417534\n",
      "Iteration 623, loss = 0.01409789\n",
      "Iteration 624, loss = 0.01405260\n",
      "Iteration 625, loss = 0.01402817\n",
      "Iteration 626, loss = 0.01396926\n",
      "Iteration 627, loss = 0.01396032\n",
      "Iteration 628, loss = 0.01387102\n",
      "Iteration 629, loss = 0.01385765\n",
      "Iteration 630, loss = 0.01379077\n",
      "Iteration 631, loss = 0.01373791\n",
      "Iteration 632, loss = 0.01374390\n",
      "Iteration 633, loss = 0.01366737\n",
      "Iteration 634, loss = 0.01363872\n",
      "Iteration 635, loss = 0.01357066\n",
      "Iteration 636, loss = 0.01356969\n",
      "Iteration 637, loss = 0.01349547\n",
      "Iteration 638, loss = 0.01347749\n",
      "Iteration 639, loss = 0.01341951\n",
      "Iteration 640, loss = 0.01336923\n",
      "Iteration 641, loss = 0.01334050\n",
      "Iteration 642, loss = 0.01329425\n",
      "Iteration 643, loss = 0.01324536\n",
      "Iteration 644, loss = 0.01325062\n",
      "Iteration 645, loss = 0.01316844\n",
      "Iteration 646, loss = 0.01313654\n",
      "Iteration 647, loss = 0.01310653\n",
      "Iteration 648, loss = 0.01305793\n",
      "Iteration 649, loss = 0.01302516\n",
      "Iteration 650, loss = 0.01301282\n",
      "Iteration 651, loss = 0.01300299\n",
      "Iteration 652, loss = 0.01290480\n",
      "Iteration 653, loss = 0.01283624\n",
      "Iteration 654, loss = 0.01282536\n",
      "Iteration 655, loss = 0.01278174\n",
      "Iteration 656, loss = 0.01273003\n",
      "Iteration 657, loss = 0.01269113\n",
      "Iteration 658, loss = 0.01266650\n",
      "Iteration 659, loss = 0.01263444\n",
      "Iteration 660, loss = 0.01258461\n",
      "Iteration 661, loss = 0.01254300\n",
      "Iteration 662, loss = 0.01252665\n",
      "Iteration 663, loss = 0.01247030\n",
      "Iteration 664, loss = 0.01248665\n",
      "Iteration 665, loss = 0.01241728\n",
      "Iteration 666, loss = 0.01240126\n",
      "Iteration 667, loss = 0.01233104\n",
      "Iteration 668, loss = 0.01232559\n",
      "Iteration 669, loss = 0.01225237\n",
      "Iteration 670, loss = 0.01228623\n",
      "Iteration 671, loss = 0.01217735\n",
      "Iteration 672, loss = 0.01214383\n",
      "Iteration 673, loss = 0.01212493\n",
      "Iteration 674, loss = 0.01208521\n",
      "Iteration 675, loss = 0.01205287\n",
      "Iteration 676, loss = 0.01201221\n",
      "Iteration 677, loss = 0.01196249\n",
      "Iteration 678, loss = 0.01195357\n",
      "Iteration 679, loss = 0.01190076\n",
      "Iteration 680, loss = 0.01188836\n",
      "Iteration 681, loss = 0.01185104\n",
      "Iteration 682, loss = 0.01179442\n",
      "Iteration 683, loss = 0.01176321\n",
      "Iteration 684, loss = 0.01175971\n",
      "Iteration 685, loss = 0.01169968\n",
      "Iteration 686, loss = 0.01165793\n",
      "Iteration 687, loss = 0.01163450\n",
      "Iteration 688, loss = 0.01162247\n",
      "Iteration 689, loss = 0.01158280\n",
      "Iteration 690, loss = 0.01155894\n",
      "Iteration 691, loss = 0.01149321\n",
      "Iteration 692, loss = 0.01146754\n",
      "Iteration 693, loss = 0.01142584\n",
      "Iteration 694, loss = 0.01143839\n",
      "Iteration 695, loss = 0.01140451\n",
      "Iteration 696, loss = 0.01136267\n",
      "Iteration 697, loss = 0.01130124\n",
      "Iteration 698, loss = 0.01128405\n",
      "Iteration 699, loss = 0.01125661\n",
      "Iteration 700, loss = 0.01123461\n",
      "Iteration 701, loss = 0.01116349\n",
      "Iteration 702, loss = 0.01114525\n",
      "Iteration 703, loss = 0.01112263\n",
      "Iteration 704, loss = 0.01109520\n",
      "Iteration 705, loss = 0.01104734\n",
      "Iteration 706, loss = 0.01100732\n",
      "Iteration 707, loss = 0.01099825\n",
      "Iteration 708, loss = 0.01097107\n",
      "Iteration 709, loss = 0.01090874\n",
      "Iteration 710, loss = 0.01088601\n",
      "Iteration 711, loss = 0.01085755\n",
      "Iteration 712, loss = 0.01082483\n",
      "Iteration 713, loss = 0.01079997\n",
      "Iteration 714, loss = 0.01075675\n",
      "Iteration 715, loss = 0.01073011\n",
      "Iteration 716, loss = 0.01070260\n",
      "Iteration 717, loss = 0.01068657\n",
      "Iteration 718, loss = 0.01065132\n",
      "Iteration 719, loss = 0.01062509\n",
      "Iteration 720, loss = 0.01059633\n",
      "Iteration 721, loss = 0.01059049\n",
      "Iteration 722, loss = 0.01056328\n",
      "Iteration 723, loss = 0.01050345\n",
      "Iteration 724, loss = 0.01049929\n",
      "Iteration 725, loss = 0.01045193\n",
      "Iteration 726, loss = 0.01041458\n",
      "Iteration 727, loss = 0.01039330\n",
      "Iteration 728, loss = 0.01036632\n",
      "Iteration 729, loss = 0.01032615\n",
      "Iteration 730, loss = 0.01031710\n",
      "Iteration 731, loss = 0.01026699\n",
      "Iteration 732, loss = 0.01026464\n",
      "Iteration 733, loss = 0.01021580\n",
      "Iteration 734, loss = 0.01019443\n",
      "Iteration 735, loss = 0.01017521\n",
      "Iteration 736, loss = 0.01015288\n",
      "Iteration 737, loss = 0.01020277\n",
      "Iteration 738, loss = 0.01009554\n",
      "Iteration 739, loss = 0.01006297\n",
      "Iteration 740, loss = 0.01003307\n",
      "Iteration 741, loss = 0.01002701\n",
      "Iteration 742, loss = 0.00998139\n",
      "Iteration 743, loss = 0.00995056\n",
      "Iteration 744, loss = 0.00993203\n",
      "Iteration 745, loss = 0.00989637\n",
      "Iteration 746, loss = 0.00987774\n",
      "Iteration 747, loss = 0.00984326\n",
      "Iteration 748, loss = 0.00981867\n",
      "Iteration 749, loss = 0.00979271\n",
      "Iteration 750, loss = 0.00978892\n",
      "Iteration 751, loss = 0.00975484\n",
      "Iteration 752, loss = 0.00969022\n",
      "Iteration 753, loss = 0.00967744\n",
      "Iteration 754, loss = 0.00966981\n",
      "Iteration 755, loss = 0.00965593\n",
      "Iteration 756, loss = 0.00964346\n",
      "Iteration 757, loss = 0.00958644\n",
      "Iteration 758, loss = 0.00956584\n",
      "Iteration 759, loss = 0.00952359\n",
      "Iteration 760, loss = 0.00950717\n",
      "Iteration 761, loss = 0.00949059\n",
      "Iteration 762, loss = 0.00946604\n",
      "Iteration 763, loss = 0.00944131\n",
      "Iteration 764, loss = 0.00939839\n",
      "Iteration 765, loss = 0.00937903\n",
      "Iteration 766, loss = 0.00935322\n",
      "Iteration 767, loss = 0.00933286\n",
      "Iteration 768, loss = 0.00930831\n",
      "Iteration 769, loss = 0.00930168\n",
      "Iteration 770, loss = 0.00925415\n",
      "Iteration 771, loss = 0.00924465\n",
      "Iteration 772, loss = 0.00921597\n",
      "Iteration 773, loss = 0.00920900\n",
      "Iteration 774, loss = 0.00917575\n",
      "Iteration 775, loss = 0.00914876\n",
      "Iteration 776, loss = 0.00912887\n",
      "Iteration 777, loss = 0.00909977\n",
      "Iteration 778, loss = 0.00907883\n",
      "Iteration 779, loss = 0.00904614\n",
      "Iteration 780, loss = 0.00900738\n",
      "Iteration 781, loss = 0.00900583\n",
      "Iteration 782, loss = 0.00897657\n",
      "Iteration 783, loss = 0.00894383\n",
      "Iteration 784, loss = 0.00894690\n",
      "Iteration 785, loss = 0.00892021\n",
      "Iteration 786, loss = 0.00887580\n",
      "Iteration 787, loss = 0.00885613\n",
      "Iteration 788, loss = 0.00883457\n",
      "Iteration 789, loss = 0.00881471\n",
      "Iteration 790, loss = 0.00880258\n",
      "Iteration 791, loss = 0.00877643\n",
      "Iteration 792, loss = 0.00874495\n",
      "Iteration 793, loss = 0.00871905\n",
      "Iteration 794, loss = 0.00869335\n",
      "Iteration 795, loss = 0.00866642\n",
      "Iteration 796, loss = 0.00865707\n",
      "Iteration 797, loss = 0.00861506\n",
      "Iteration 798, loss = 0.00858501\n",
      "Iteration 799, loss = 0.00857697\n",
      "Iteration 800, loss = 0.00858880\n",
      "Iteration 801, loss = 0.00856572\n",
      "Iteration 802, loss = 0.00848876\n",
      "Iteration 803, loss = 0.00848437\n",
      "Iteration 804, loss = 0.00849832\n",
      "Iteration 805, loss = 0.00844989\n",
      "Iteration 806, loss = 0.00841563\n",
      "Iteration 807, loss = 0.00838407\n",
      "Iteration 808, loss = 0.00837354\n",
      "Iteration 809, loss = 0.00835168\n",
      "Iteration 810, loss = 0.00830250\n",
      "Iteration 811, loss = 0.00829455\n",
      "Iteration 812, loss = 0.00829675\n",
      "Iteration 813, loss = 0.00825702\n",
      "Iteration 814, loss = 0.00823362\n",
      "Iteration 815, loss = 0.00826072\n",
      "Iteration 816, loss = 0.00820189\n",
      "Iteration 817, loss = 0.00818999\n",
      "Iteration 818, loss = 0.00814479\n",
      "Iteration 819, loss = 0.00814012\n",
      "Iteration 820, loss = 0.00812038\n",
      "Iteration 821, loss = 0.00810880\n",
      "Iteration 822, loss = 0.00807012\n",
      "Iteration 823, loss = 0.00804946\n",
      "Iteration 824, loss = 0.00803337\n",
      "Iteration 825, loss = 0.00802026\n",
      "Iteration 826, loss = 0.00800037\n",
      "Iteration 827, loss = 0.00802295\n",
      "Iteration 828, loss = 0.00794532\n",
      "Iteration 829, loss = 0.00794877\n",
      "Iteration 830, loss = 0.00791015\n",
      "Iteration 831, loss = 0.00787344\n",
      "Iteration 832, loss = 0.00785543\n",
      "Iteration 833, loss = 0.00784966\n",
      "Iteration 834, loss = 0.00784647\n",
      "Iteration 835, loss = 0.00783338\n",
      "Iteration 836, loss = 0.00779015\n",
      "Iteration 837, loss = 0.00778184\n",
      "Iteration 838, loss = 0.00776381\n",
      "Iteration 839, loss = 0.00771682\n",
      "Iteration 840, loss = 0.00771576\n",
      "Iteration 841, loss = 0.00770108\n",
      "Iteration 842, loss = 0.00767988\n",
      "Iteration 843, loss = 0.00767260\n",
      "Iteration 844, loss = 0.00764707\n",
      "Iteration 845, loss = 0.00762252\n",
      "Iteration 846, loss = 0.00760252\n",
      "Iteration 847, loss = 0.00757132\n",
      "Iteration 848, loss = 0.00754919\n",
      "Iteration 849, loss = 0.00754447\n",
      "Iteration 850, loss = 0.00754350\n",
      "Iteration 851, loss = 0.00748538\n",
      "Iteration 852, loss = 0.00748014\n",
      "Iteration 853, loss = 0.00745963\n",
      "Iteration 854, loss = 0.00744950\n",
      "Iteration 855, loss = 0.00742783\n",
      "Iteration 856, loss = 0.00742609\n",
      "Iteration 857, loss = 0.00738986\n",
      "Iteration 858, loss = 0.00736355\n",
      "Iteration 859, loss = 0.00737601\n",
      "Iteration 860, loss = 0.00735591\n",
      "Iteration 861, loss = 0.00733063\n",
      "Iteration 862, loss = 0.00730587\n",
      "Iteration 863, loss = 0.00727690\n",
      "Iteration 864, loss = 0.00728266\n",
      "Iteration 865, loss = 0.00725608\n",
      "Iteration 866, loss = 0.00723933\n",
      "Iteration 867, loss = 0.00723059\n",
      "Iteration 868, loss = 0.00720662\n",
      "Iteration 869, loss = 0.00718180\n",
      "Iteration 870, loss = 0.00717557\n",
      "Iteration 871, loss = 0.00715657\n",
      "Iteration 872, loss = 0.00715775\n",
      "Iteration 873, loss = 0.00714245\n",
      "Iteration 874, loss = 0.00711066\n",
      "Iteration 875, loss = 0.00708806\n",
      "Iteration 876, loss = 0.00708714\n",
      "Iteration 877, loss = 0.00705425\n",
      "Iteration 878, loss = 0.00704939\n",
      "Iteration 879, loss = 0.00700895\n",
      "Iteration 880, loss = 0.00703065\n",
      "Iteration 881, loss = 0.00697291\n",
      "Iteration 882, loss = 0.00697456\n",
      "Iteration 883, loss = 0.00694620\n",
      "Iteration 884, loss = 0.00692848\n",
      "Iteration 885, loss = 0.00694057\n",
      "Iteration 886, loss = 0.00689460\n",
      "Iteration 887, loss = 0.00688856\n",
      "Iteration 888, loss = 0.00686881\n",
      "Iteration 889, loss = 0.00683643\n",
      "Iteration 890, loss = 0.00687012\n",
      "Iteration 891, loss = 0.00683549\n",
      "Iteration 892, loss = 0.00679682\n",
      "Iteration 893, loss = 0.00681983\n",
      "Iteration 894, loss = 0.00675521\n",
      "Iteration 895, loss = 0.00675017\n",
      "Iteration 896, loss = 0.00673377\n",
      "Iteration 897, loss = 0.00670745\n",
      "Iteration 898, loss = 0.00669049\n",
      "Iteration 899, loss = 0.00667741\n",
      "Iteration 900, loss = 0.00666391\n",
      "Iteration 901, loss = 0.00665028\n",
      "Iteration 902, loss = 0.00664010\n",
      "Iteration 903, loss = 0.00662151\n",
      "Iteration 904, loss = 0.00659677\n",
      "Iteration 905, loss = 0.00658987\n",
      "Iteration 906, loss = 0.00657888\n",
      "Iteration 907, loss = 0.00655660\n",
      "Iteration 908, loss = 0.00655969\n",
      "Iteration 909, loss = 0.00650804\n",
      "Iteration 910, loss = 0.00649931\n",
      "Iteration 911, loss = 0.00649055\n",
      "Iteration 912, loss = 0.00648534\n",
      "Iteration 913, loss = 0.00646233\n",
      "Iteration 914, loss = 0.00646982\n",
      "Iteration 915, loss = 0.00642824\n",
      "Iteration 916, loss = 0.00644649\n",
      "Iteration 917, loss = 0.00638037\n",
      "Iteration 918, loss = 0.00636922\n",
      "Iteration 919, loss = 0.00638268\n",
      "Iteration 920, loss = 0.00635360\n",
      "Iteration 921, loss = 0.00631203\n",
      "Iteration 922, loss = 0.00630934\n",
      "Iteration 923, loss = 0.00629776\n",
      "Iteration 924, loss = 0.00626034\n",
      "Iteration 925, loss = 0.00625631\n",
      "Iteration 926, loss = 0.00624824\n",
      "Iteration 927, loss = 0.00624650\n",
      "Iteration 928, loss = 0.00621269\n",
      "Iteration 929, loss = 0.00618662\n",
      "Iteration 930, loss = 0.00618761\n",
      "Iteration 931, loss = 0.00618415\n",
      "Iteration 932, loss = 0.00615069\n",
      "Iteration 933, loss = 0.00612883\n",
      "Iteration 934, loss = 0.00613728\n",
      "Iteration 935, loss = 0.00612443\n",
      "Iteration 936, loss = 0.00609178\n",
      "Iteration 937, loss = 0.00606324\n",
      "Iteration 938, loss = 0.00606352\n",
      "Iteration 939, loss = 0.00602080\n",
      "Iteration 940, loss = 0.00600547\n",
      "Iteration 941, loss = 0.00600567\n",
      "Iteration 942, loss = 0.00598158\n",
      "Iteration 943, loss = 0.00598329\n",
      "Iteration 944, loss = 0.00597584\n",
      "Iteration 945, loss = 0.00593982\n",
      "Iteration 946, loss = 0.00593236\n",
      "Iteration 947, loss = 0.00591295\n",
      "Iteration 948, loss = 0.00589608\n",
      "Iteration 949, loss = 0.00587628\n",
      "Iteration 950, loss = 0.00584951\n",
      "Iteration 951, loss = 0.00583364\n",
      "Iteration 952, loss = 0.00583164\n",
      "Iteration 953, loss = 0.00585120\n",
      "Iteration 954, loss = 0.00579174\n",
      "Iteration 955, loss = 0.00580615\n",
      "Iteration 956, loss = 0.00577282\n",
      "Iteration 957, loss = 0.00576956\n",
      "Iteration 958, loss = 0.00575600\n",
      "Iteration 959, loss = 0.00573999\n",
      "Iteration 960, loss = 0.00575708\n",
      "Iteration 961, loss = 0.00569888\n",
      "Iteration 962, loss = 0.00571238\n",
      "Iteration 963, loss = 0.00567308\n",
      "Iteration 964, loss = 0.00568596\n",
      "Iteration 965, loss = 0.00566737\n",
      "Iteration 966, loss = 0.00563434\n",
      "Iteration 967, loss = 0.00562686\n",
      "Iteration 968, loss = 0.00562455\n",
      "Iteration 969, loss = 0.00558183\n",
      "Iteration 970, loss = 0.00557391\n",
      "Iteration 971, loss = 0.00558542\n",
      "Iteration 972, loss = 0.00556916\n",
      "Iteration 973, loss = 0.00553829\n",
      "Iteration 974, loss = 0.00552533\n",
      "Iteration 975, loss = 0.00552287\n",
      "Iteration 976, loss = 0.00548269\n",
      "Iteration 977, loss = 0.00547447\n",
      "Iteration 978, loss = 0.00548509\n",
      "Iteration 979, loss = 0.00546255\n",
      "Iteration 980, loss = 0.00545773\n",
      "Iteration 981, loss = 0.00543205\n",
      "Iteration 982, loss = 0.00542034\n",
      "Iteration 983, loss = 0.00541897\n",
      "Iteration 984, loss = 0.00541675\n",
      "Iteration 985, loss = 0.00537866\n",
      "Iteration 986, loss = 0.00537170\n",
      "Iteration 987, loss = 0.00535276\n",
      "Iteration 988, loss = 0.00535483\n",
      "Iteration 989, loss = 0.00532645\n",
      "Iteration 990, loss = 0.00533437\n",
      "Iteration 991, loss = 0.00530644\n",
      "Iteration 992, loss = 0.00527933\n",
      "Iteration 993, loss = 0.00528904\n",
      "Iteration 994, loss = 0.00525630\n",
      "Iteration 995, loss = 0.00523078\n",
      "Iteration 996, loss = 0.00522986\n",
      "Iteration 997, loss = 0.00521759\n",
      "Iteration 998, loss = 0.00525807\n",
      "Iteration 999, loss = 0.00519176\n",
      "Iteration 1000, loss = 0.00517998\n",
      "Iteration 1001, loss = 0.00517157\n",
      "Iteration 1002, loss = 0.00518759\n",
      "Iteration 1003, loss = 0.00514221\n",
      "Iteration 1004, loss = 0.00513475\n",
      "Iteration 1005, loss = 0.00514730\n",
      "Iteration 1006, loss = 0.00511136\n",
      "Iteration 1007, loss = 0.00509176\n",
      "Iteration 1008, loss = 0.00509601\n",
      "Iteration 1009, loss = 0.00508683\n",
      "Iteration 1010, loss = 0.00508380\n",
      "Iteration 1011, loss = 0.00505937\n",
      "Iteration 1012, loss = 0.00504831\n",
      "Iteration 1013, loss = 0.00505242\n",
      "Iteration 1014, loss = 0.00505242\n",
      "Iteration 1015, loss = 0.00501274\n",
      "Iteration 1016, loss = 0.00501040\n",
      "Iteration 1017, loss = 0.00498949\n",
      "Iteration 1018, loss = 0.00501116\n",
      "Iteration 1019, loss = 0.00497877\n",
      "Iteration 1020, loss = 0.00495191\n",
      "Iteration 1021, loss = 0.00496613\n",
      "Iteration 1022, loss = 0.00494581\n",
      "Iteration 1023, loss = 0.00492211\n",
      "Iteration 1024, loss = 0.00492026\n",
      "Iteration 1025, loss = 0.00488393\n",
      "Iteration 1026, loss = 0.00489547\n",
      "Iteration 1027, loss = 0.00487040\n",
      "Iteration 1028, loss = 0.00486615\n",
      "Iteration 1029, loss = 0.00486372\n",
      "Iteration 1030, loss = 0.00485098\n",
      "Iteration 1031, loss = 0.00482841\n",
      "Iteration 1032, loss = 0.00482850\n",
      "Iteration 1033, loss = 0.00480985\n",
      "Iteration 1034, loss = 0.00478475\n",
      "Iteration 1035, loss = 0.00477562\n",
      "Iteration 1036, loss = 0.00477568\n",
      "Iteration 1037, loss = 0.00477880\n",
      "Iteration 1038, loss = 0.00473779\n",
      "Iteration 1039, loss = 0.00472597\n",
      "Iteration 1040, loss = 0.00472072\n",
      "Iteration 1041, loss = 0.00472624\n",
      "Iteration 1042, loss = 0.00470199\n",
      "Iteration 1043, loss = 0.00468512\n",
      "Iteration 1044, loss = 0.00468017\n",
      "Iteration 1045, loss = 0.00467357\n",
      "Iteration 1046, loss = 0.00466146\n",
      "Iteration 1047, loss = 0.00468148\n",
      "Iteration 1048, loss = 0.00465427\n",
      "Iteration 1049, loss = 0.00464177\n",
      "Iteration 1050, loss = 0.00462462\n",
      "Iteration 1051, loss = 0.00461110\n",
      "Iteration 1052, loss = 0.00459265\n",
      "Iteration 1053, loss = 0.00459655\n",
      "Iteration 1054, loss = 0.00458188\n",
      "Iteration 1055, loss = 0.00457984\n",
      "Iteration 1056, loss = 0.00457124\n",
      "Iteration 1057, loss = 0.00455574\n",
      "Iteration 1058, loss = 0.00453960\n",
      "Iteration 1059, loss = 0.00454701\n",
      "Iteration 1060, loss = 0.00453006\n",
      "Iteration 1061, loss = 0.00451091\n",
      "Iteration 1062, loss = 0.00450573\n",
      "Iteration 1063, loss = 0.00448880\n",
      "Iteration 1064, loss = 0.00447745\n",
      "Iteration 1065, loss = 0.00448288\n",
      "Iteration 1066, loss = 0.00446462\n",
      "Iteration 1067, loss = 0.00444540\n",
      "Iteration 1068, loss = 0.00446017\n",
      "Iteration 1069, loss = 0.00443083\n",
      "Iteration 1070, loss = 0.00442861\n",
      "Iteration 1071, loss = 0.00442367\n",
      "Iteration 1072, loss = 0.00440384\n",
      "Iteration 1073, loss = 0.00438115\n",
      "Iteration 1074, loss = 0.00439850\n",
      "Iteration 1075, loss = 0.00438538\n",
      "Iteration 1076, loss = 0.00437147\n",
      "Iteration 1077, loss = 0.00433302\n",
      "Iteration 1078, loss = 0.00434857\n",
      "Iteration 1079, loss = 0.00434309\n",
      "Iteration 1080, loss = 0.00433269\n",
      "Iteration 1081, loss = 0.00430324\n",
      "Iteration 1082, loss = 0.00431372\n",
      "Iteration 1083, loss = 0.00430132\n",
      "Iteration 1084, loss = 0.00428221\n",
      "Iteration 1085, loss = 0.00427774\n",
      "Iteration 1086, loss = 0.00426447\n",
      "Iteration 1087, loss = 0.00424584\n",
      "Iteration 1088, loss = 0.00426198\n",
      "Iteration 1089, loss = 0.00422917\n",
      "Iteration 1090, loss = 0.00424391\n",
      "Iteration 1091, loss = 0.00424974\n",
      "Iteration 1092, loss = 0.00421559\n",
      "Iteration 1093, loss = 0.00425036\n",
      "Iteration 1094, loss = 0.00421701\n",
      "Iteration 1095, loss = 0.00418047\n",
      "Iteration 1096, loss = 0.00418840\n",
      "Iteration 1097, loss = 0.00416556\n",
      "Iteration 1098, loss = 0.00417356\n",
      "Iteration 1099, loss = 0.00415003\n",
      "Iteration 1100, loss = 0.00414139\n",
      "Iteration 1101, loss = 0.00412586\n",
      "Iteration 1102, loss = 0.00411433\n",
      "Iteration 1103, loss = 0.00411239\n",
      "Iteration 1104, loss = 0.00409952\n",
      "Iteration 1105, loss = 0.00412216\n",
      "Iteration 1106, loss = 0.00408600\n",
      "Iteration 1107, loss = 0.00407475\n",
      "Iteration 1108, loss = 0.00407160\n",
      "Iteration 1109, loss = 0.00406655\n",
      "Iteration 1110, loss = 0.00409471\n",
      "Iteration 1111, loss = 0.00404806\n",
      "Iteration 1112, loss = 0.00404569\n",
      "Iteration 1113, loss = 0.00402022\n",
      "Iteration 1114, loss = 0.00403720\n",
      "Iteration 1115, loss = 0.00401529\n",
      "Iteration 1116, loss = 0.00402348\n",
      "Iteration 1117, loss = 0.00399671\n",
      "Iteration 1118, loss = 0.00398380\n",
      "Iteration 1119, loss = 0.00396731\n",
      "Iteration 1120, loss = 0.00399637\n",
      "Iteration 1121, loss = 0.00397450\n",
      "Iteration 1122, loss = 0.00395372\n",
      "Iteration 1123, loss = 0.00394724\n",
      "Iteration 1124, loss = 0.00396084\n",
      "Iteration 1125, loss = 0.00393855\n",
      "Iteration 1126, loss = 0.00392146\n",
      "Iteration 1127, loss = 0.00392083\n",
      "Iteration 1128, loss = 0.00392346\n",
      "Iteration 1129, loss = 0.00389037\n",
      "Iteration 1130, loss = 0.00387810\n",
      "Iteration 1131, loss = 0.00387635\n",
      "Iteration 1132, loss = 0.00388835\n",
      "Iteration 1133, loss = 0.00388704\n",
      "Iteration 1134, loss = 0.00386538\n",
      "Iteration 1135, loss = 0.00384014\n",
      "Iteration 1136, loss = 0.00384905\n",
      "Iteration 1137, loss = 0.00386124\n",
      "Iteration 1138, loss = 0.00382686\n",
      "Iteration 1139, loss = 0.00380884\n",
      "Iteration 1140, loss = 0.00384539\n",
      "Iteration 1141, loss = 0.00380344\n",
      "Iteration 1142, loss = 0.00380633\n",
      "Iteration 1143, loss = 0.00379078\n",
      "Iteration 1144, loss = 0.00378770\n",
      "Iteration 1145, loss = 0.00378430\n",
      "Iteration 1146, loss = 0.00375646\n",
      "Iteration 1147, loss = 0.00375376\n",
      "Iteration 1148, loss = 0.00376715\n",
      "Iteration 1149, loss = 0.00374892\n",
      "Iteration 1150, loss = 0.00377046\n",
      "Iteration 1151, loss = 0.00371404\n",
      "Iteration 1152, loss = 0.00370476\n",
      "Iteration 1153, loss = 0.00370979\n",
      "Iteration 1154, loss = 0.00372404\n",
      "Iteration 1155, loss = 0.00370452\n",
      "Iteration 1156, loss = 0.00368186\n",
      "Iteration 1157, loss = 0.00369985\n",
      "Iteration 1158, loss = 0.00370102\n",
      "Iteration 1159, loss = 0.00366347\n",
      "Iteration 1160, loss = 0.00366454\n",
      "Iteration 1161, loss = 0.00365482\n",
      "Iteration 1162, loss = 0.00365968\n",
      "Iteration 1163, loss = 0.00363352\n",
      "Iteration 1164, loss = 0.00363521\n",
      "Iteration 1165, loss = 0.00363025\n",
      "Iteration 1166, loss = 0.00362853\n",
      "Iteration 1167, loss = 0.00362104\n",
      "Iteration 1168, loss = 0.00361563\n",
      "Iteration 1169, loss = 0.00360354\n",
      "Iteration 1170, loss = 0.00359459\n",
      "Iteration 1171, loss = 0.00357989\n",
      "Iteration 1172, loss = 0.00360473\n",
      "Iteration 1173, loss = 0.00357743\n",
      "Iteration 1174, loss = 0.00356757\n",
      "Iteration 1175, loss = 0.00356471\n",
      "Iteration 1176, loss = 0.00354248\n",
      "Iteration 1177, loss = 0.00357009\n",
      "Iteration 1178, loss = 0.00354146\n",
      "Iteration 1179, loss = 0.00352817\n",
      "Iteration 1180, loss = 0.00352853\n",
      "Iteration 1181, loss = 0.00351414\n",
      "Iteration 1182, loss = 0.00352185\n",
      "Iteration 1183, loss = 0.00350545\n",
      "Iteration 1184, loss = 0.00350361\n",
      "Iteration 1185, loss = 0.00348268\n",
      "Iteration 1186, loss = 0.00350049\n",
      "Iteration 1187, loss = 0.00348161\n",
      "Iteration 1188, loss = 0.00346076\n",
      "Iteration 1189, loss = 0.00346967\n",
      "Iteration 1190, loss = 0.00347432\n",
      "Iteration 1191, loss = 0.00345107\n",
      "Iteration 1192, loss = 0.00344590\n",
      "Iteration 1193, loss = 0.00345729\n",
      "Iteration 1194, loss = 0.00345083\n",
      "Iteration 1195, loss = 0.00343031\n",
      "Iteration 1196, loss = 0.00343159\n",
      "Iteration 1197, loss = 0.00341310\n",
      "Iteration 1198, loss = 0.00343202\n",
      "Iteration 1199, loss = 0.00340884\n",
      "Iteration 1200, loss = 0.00341580\n",
      "Iteration 1201, loss = 0.00338880\n",
      "Iteration 1202, loss = 0.00336933\n",
      "Iteration 1203, loss = 0.00337813\n",
      "Iteration 1204, loss = 0.00336149\n",
      "Iteration 1205, loss = 0.00336288\n",
      "Iteration 1206, loss = 0.00335395\n",
      "Iteration 1207, loss = 0.00335642\n",
      "Iteration 1208, loss = 0.00333991\n",
      "Iteration 1209, loss = 0.00333059\n",
      "Iteration 1210, loss = 0.00335916\n",
      "Iteration 1211, loss = 0.00333136\n",
      "Iteration 1212, loss = 0.00331790\n",
      "Iteration 1213, loss = 0.00332639\n",
      "Iteration 1214, loss = 0.00332970\n",
      "Iteration 1215, loss = 0.00330636\n",
      "Iteration 1216, loss = 0.00329084\n",
      "Iteration 1217, loss = 0.00329221\n",
      "Iteration 1218, loss = 0.00326768\n",
      "Iteration 1219, loss = 0.00328189\n",
      "Iteration 1220, loss = 0.00328121\n",
      "Iteration 1221, loss = 0.00327774\n",
      "Iteration 1222, loss = 0.00326212\n",
      "Iteration 1223, loss = 0.00324663\n",
      "Iteration 1224, loss = 0.00326281\n",
      "Iteration 1225, loss = 0.00326030\n",
      "Iteration 1226, loss = 0.00324957\n",
      "Iteration 1227, loss = 0.00323593\n",
      "Iteration 1228, loss = 0.00322973\n",
      "Iteration 1229, loss = 0.00326683\n",
      "Iteration 1230, loss = 0.00320873\n",
      "Iteration 1231, loss = 0.00322888\n",
      "Iteration 1232, loss = 0.00320579\n",
      "Iteration 1233, loss = 0.00322829\n",
      "Iteration 1234, loss = 0.00320236\n",
      "Iteration 1235, loss = 0.00319073\n",
      "Iteration 1236, loss = 0.00318611\n",
      "Iteration 1237, loss = 0.00317870\n",
      "Iteration 1238, loss = 0.00319510\n",
      "Iteration 1239, loss = 0.00316515\n",
      "Iteration 1240, loss = 0.00315673\n",
      "Iteration 1241, loss = 0.00315138\n",
      "Iteration 1242, loss = 0.00314296\n",
      "Iteration 1243, loss = 0.00318086\n",
      "Iteration 1244, loss = 0.00316032\n",
      "Iteration 1245, loss = 0.00314534\n",
      "Iteration 1246, loss = 0.00311566\n",
      "Iteration 1247, loss = 0.00312010\n",
      "Iteration 1248, loss = 0.00312989\n",
      "Iteration 1249, loss = 0.00312942\n",
      "Iteration 1250, loss = 0.00310227\n",
      "Iteration 1251, loss = 0.00311197\n",
      "Iteration 1252, loss = 0.00311253\n",
      "Iteration 1253, loss = 0.00310573\n",
      "Iteration 1254, loss = 0.00309042\n",
      "Iteration 1255, loss = 0.00309294\n",
      "Iteration 1256, loss = 0.00307118\n",
      "Iteration 1257, loss = 0.00308001\n",
      "Iteration 1258, loss = 0.00306532\n",
      "Iteration 1259, loss = 0.00305401\n",
      "Iteration 1260, loss = 0.00308148\n",
      "Iteration 1261, loss = 0.00305691\n",
      "Iteration 1262, loss = 0.00304902\n",
      "Iteration 1263, loss = 0.00305940\n",
      "Iteration 1264, loss = 0.00304636\n",
      "Iteration 1265, loss = 0.00304329\n",
      "Iteration 1266, loss = 0.00302232\n",
      "Iteration 1267, loss = 0.00302709\n",
      "Iteration 1268, loss = 0.00302178\n",
      "Iteration 1269, loss = 0.00301023\n",
      "Iteration 1270, loss = 0.00300394\n",
      "Iteration 1271, loss = 0.00300936\n",
      "Iteration 1272, loss = 0.00299962\n",
      "Iteration 1273, loss = 0.00299001\n",
      "Iteration 1274, loss = 0.00301280\n",
      "Iteration 1275, loss = 0.00301791\n",
      "Iteration 1276, loss = 0.00299410\n",
      "Iteration 1277, loss = 0.00299534\n",
      "Iteration 1278, loss = 0.00300263\n",
      "Iteration 1279, loss = 0.00297026\n",
      "Iteration 1280, loss = 0.00297107\n",
      "Iteration 1281, loss = 0.00295618\n",
      "Iteration 1282, loss = 0.00294261\n",
      "Iteration 1283, loss = 0.00293957\n",
      "Iteration 1284, loss = 0.00295004\n",
      "Iteration 1285, loss = 0.00293515\n",
      "Iteration 1286, loss = 0.00291439\n",
      "Iteration 1287, loss = 0.00294145\n",
      "Iteration 1288, loss = 0.00293669\n",
      "Iteration 1289, loss = 0.00294969\n",
      "Iteration 1290, loss = 0.00296115\n",
      "Iteration 1291, loss = 0.00293697\n",
      "Iteration 1292, loss = 0.00290752\n",
      "Iteration 1293, loss = 0.00294067\n",
      "Iteration 1294, loss = 0.00290834\n",
      "Iteration 1295, loss = 0.00290925\n",
      "Iteration 1296, loss = 0.00289187\n",
      "Iteration 1297, loss = 0.00290336\n",
      "Iteration 1298, loss = 0.00287444\n",
      "Iteration 1299, loss = 0.00286835\n",
      "Iteration 1300, loss = 0.00286406\n",
      "Iteration 1301, loss = 0.00287695\n",
      "Iteration 1302, loss = 0.00286298\n",
      "Iteration 1303, loss = 0.00287859\n",
      "Iteration 1304, loss = 0.00285296\n",
      "Iteration 1305, loss = 0.00283466\n",
      "Iteration 1306, loss = 0.00284900\n",
      "Iteration 1307, loss = 0.00285845\n",
      "Iteration 1308, loss = 0.00284214\n",
      "Iteration 1309, loss = 0.00285411\n",
      "Iteration 1310, loss = 0.00281976\n",
      "Iteration 1311, loss = 0.00282848\n",
      "Iteration 1312, loss = 0.00282128\n",
      "Iteration 1313, loss = 0.00282248\n",
      "Iteration 1314, loss = 0.00283956\n",
      "Iteration 1315, loss = 0.00280332\n",
      "Iteration 1316, loss = 0.00280355\n",
      "Iteration 1317, loss = 0.00280269\n",
      "Iteration 1318, loss = 0.00280201\n",
      "Iteration 1319, loss = 0.00279466\n",
      "Iteration 1320, loss = 0.00280734\n",
      "Iteration 1321, loss = 0.00278858\n",
      "Iteration 1322, loss = 0.00280822\n",
      "Iteration 1323, loss = 0.00280220\n",
      "Iteration 1324, loss = 0.00279608\n",
      "Iteration 1325, loss = 0.00277213\n",
      "Iteration 1326, loss = 0.00276889\n",
      "Iteration 1327, loss = 0.00277335\n",
      "Iteration 1328, loss = 0.00277373\n",
      "Iteration 1329, loss = 0.00275742\n",
      "Iteration 1330, loss = 0.00275442\n",
      "Iteration 1331, loss = 0.00273858\n",
      "Iteration 1332, loss = 0.00274003\n",
      "Iteration 1333, loss = 0.00274094\n",
      "Iteration 1334, loss = 0.00274998\n",
      "Iteration 1335, loss = 0.00274197\n",
      "Iteration 1336, loss = 0.00272830\n",
      "Iteration 1337, loss = 0.00274954\n",
      "Iteration 1338, loss = 0.00272243\n",
      "Iteration 1339, loss = 0.00274446\n",
      "Iteration 1340, loss = 0.00272892\n",
      "Iteration 1341, loss = 0.00270133\n",
      "Iteration 1342, loss = 0.00269945\n",
      "Iteration 1343, loss = 0.00272467\n",
      "Iteration 1344, loss = 0.00270281\n",
      "Iteration 1345, loss = 0.00271394\n",
      "Iteration 1346, loss = 0.00267714\n",
      "Iteration 1347, loss = 0.00266443\n",
      "Iteration 1348, loss = 0.00270204\n",
      "Iteration 1349, loss = 0.00268481\n",
      "Iteration 1350, loss = 0.00267426\n",
      "Iteration 1351, loss = 0.00266542\n",
      "Iteration 1352, loss = 0.00266367\n",
      "Iteration 1353, loss = 0.00264188\n",
      "Iteration 1354, loss = 0.00267053\n",
      "Iteration 1355, loss = 0.00264334\n",
      "Iteration 1356, loss = 0.00267078\n",
      "Iteration 1357, loss = 0.00264397\n",
      "Iteration 1358, loss = 0.00263588\n",
      "Iteration 1359, loss = 0.00263037\n",
      "Iteration 1360, loss = 0.00262754\n",
      "Iteration 1361, loss = 0.00262423\n",
      "Iteration 1362, loss = 0.00264993\n",
      "Iteration 1363, loss = 0.00261883\n",
      "Iteration 1364, loss = 0.00262352\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3  --> 100 --> 100 --> 1\n",
    "# 3  --> 2 --> 2 --> 1\n",
    "rede_neural_credit = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000100,\n",
    "                                   solver='adam',\n",
    "                                   activation='relu',\n",
    "                                   hidden_layer_sizes=(2,2))\n",
    "rede_neural_credit.fit(X_credit_treinamento, y_credit_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = rede_neural_credit.predict(X_credit_teste)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_credit_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy_score(y_credit_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "cm = ConfusionMatrix(rede_neural_credit)\n",
    "cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
    "cm.score(X_credit_teste, y_credit_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       0.98      1.00      0.99        64\n",
      "\n",
      "    accuracy                           1.00       500\n",
      "   macro avg       0.99      1.00      1.00       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_credit_teste, previsoes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Bases de dados/census.pkl', 'rb') as f:\n",
    "    X_census_treinamento, y_census_treinamento, X_census_teste, y_census_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27676, 108), (27676,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_census_treinamento.shape, y_census_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4885, 108), (4885,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_census_teste.shape, y_census_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(108 +1) /2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.38177886\n",
      "Iteration 2, loss = 0.32592999\n",
      "Iteration 3, loss = 0.31472684\n",
      "Iteration 4, loss = 0.30790643\n",
      "Iteration 5, loss = 0.30302278\n",
      "Iteration 6, loss = 0.29858239\n",
      "Iteration 7, loss = 0.29551893\n",
      "Iteration 8, loss = 0.29232461\n",
      "Iteration 9, loss = 0.29050371\n",
      "Iteration 10, loss = 0.28784323\n",
      "Iteration 11, loss = 0.28586588\n",
      "Iteration 12, loss = 0.28412581\n",
      "Iteration 13, loss = 0.28217946\n",
      "Iteration 14, loss = 0.28034960\n",
      "Iteration 15, loss = 0.27928694\n",
      "Iteration 16, loss = 0.27726738\n",
      "Iteration 17, loss = 0.27500639\n",
      "Iteration 18, loss = 0.27430943\n",
      "Iteration 19, loss = 0.27246179\n",
      "Iteration 20, loss = 0.27159839\n",
      "Iteration 21, loss = 0.26933544\n",
      "Iteration 22, loss = 0.26781914\n",
      "Iteration 23, loss = 0.26663441\n",
      "Iteration 24, loss = 0.26485814\n",
      "Iteration 25, loss = 0.26342022\n",
      "Iteration 26, loss = 0.26252770\n",
      "Iteration 27, loss = 0.26081454\n",
      "Iteration 28, loss = 0.25951050\n",
      "Iteration 29, loss = 0.25839976\n",
      "Iteration 30, loss = 0.25714965\n",
      "Iteration 31, loss = 0.25576056\n",
      "Iteration 32, loss = 0.25491672\n",
      "Iteration 33, loss = 0.25345179\n",
      "Iteration 34, loss = 0.25225591\n",
      "Iteration 35, loss = 0.25096389\n",
      "Iteration 36, loss = 0.24953144\n",
      "Iteration 37, loss = 0.24845028\n",
      "Iteration 38, loss = 0.24717876\n",
      "Iteration 39, loss = 0.24664542\n",
      "Iteration 40, loss = 0.24568664\n",
      "Iteration 41, loss = 0.24355096\n",
      "Iteration 42, loss = 0.24294870\n",
      "Iteration 43, loss = 0.24164081\n",
      "Iteration 44, loss = 0.24134683\n",
      "Iteration 45, loss = 0.23942944\n",
      "Iteration 46, loss = 0.23804312\n",
      "Iteration 47, loss = 0.23743751\n",
      "Iteration 48, loss = 0.23661612\n",
      "Iteration 49, loss = 0.23512427\n",
      "Iteration 50, loss = 0.23467512\n",
      "Iteration 51, loss = 0.23308627\n",
      "Iteration 52, loss = 0.23235252\n",
      "Iteration 53, loss = 0.23268857\n",
      "Iteration 54, loss = 0.23028977\n",
      "Iteration 55, loss = 0.22934748\n",
      "Iteration 56, loss = 0.22866863\n",
      "Iteration 57, loss = 0.22762494\n",
      "Iteration 58, loss = 0.22667553\n",
      "Iteration 59, loss = 0.22627278\n",
      "Iteration 60, loss = 0.22483324\n",
      "Iteration 61, loss = 0.22374681\n",
      "Iteration 62, loss = 0.22262187\n",
      "Iteration 63, loss = 0.22245856\n",
      "Iteration 64, loss = 0.22190394\n",
      "Iteration 65, loss = 0.22020680\n",
      "Iteration 66, loss = 0.22002909\n",
      "Iteration 67, loss = 0.21841400\n",
      "Iteration 68, loss = 0.21813332\n",
      "Iteration 69, loss = 0.21796633\n",
      "Iteration 70, loss = 0.21785340\n",
      "Iteration 71, loss = 0.21665618\n",
      "Iteration 72, loss = 0.21549156\n",
      "Iteration 73, loss = 0.21479562\n",
      "Iteration 74, loss = 0.21341320\n",
      "Iteration 75, loss = 0.21344163\n",
      "Iteration 76, loss = 0.21292869\n",
      "Iteration 77, loss = 0.21164219\n",
      "Iteration 78, loss = 0.21123205\n",
      "Iteration 79, loss = 0.20968293\n",
      "Iteration 80, loss = 0.20989131\n",
      "Iteration 81, loss = 0.21017636\n",
      "Iteration 82, loss = 0.20840875\n",
      "Iteration 83, loss = 0.20724680\n",
      "Iteration 84, loss = 0.20689748\n",
      "Iteration 85, loss = 0.20622543\n",
      "Iteration 86, loss = 0.20507226\n",
      "Iteration 87, loss = 0.20458529\n",
      "Iteration 88, loss = 0.20395327\n",
      "Iteration 89, loss = 0.20459780\n",
      "Iteration 90, loss = 0.20297267\n",
      "Iteration 91, loss = 0.20209516\n",
      "Iteration 92, loss = 0.20207012\n",
      "Iteration 93, loss = 0.20154134\n",
      "Iteration 94, loss = 0.20151336\n",
      "Iteration 95, loss = 0.20165527\n",
      "Iteration 96, loss = 0.19992748\n",
      "Iteration 97, loss = 0.19830021\n",
      "Iteration 98, loss = 0.19836691\n",
      "Iteration 99, loss = 0.19849404\n",
      "Iteration 100, loss = 0.19762945\n",
      "Iteration 101, loss = 0.19672806\n",
      "Iteration 102, loss = 0.19728025\n",
      "Iteration 103, loss = 0.19646714\n",
      "Iteration 104, loss = 0.19598987\n",
      "Iteration 105, loss = 0.19452004\n",
      "Iteration 106, loss = 0.19424985\n",
      "Iteration 107, loss = 0.19468084\n",
      "Iteration 108, loss = 0.19386190\n",
      "Iteration 109, loss = 0.19375608\n",
      "Iteration 110, loss = 0.19319404\n",
      "Iteration 111, loss = 0.19384604\n",
      "Iteration 112, loss = 0.19299885\n",
      "Iteration 113, loss = 0.19251502\n",
      "Iteration 114, loss = 0.19113402\n",
      "Iteration 115, loss = 0.19101459\n",
      "Iteration 116, loss = 0.19016580\n",
      "Iteration 117, loss = 0.18910885\n",
      "Iteration 118, loss = 0.18987886\n",
      "Iteration 119, loss = 0.18998819\n",
      "Iteration 120, loss = 0.18804730\n",
      "Iteration 121, loss = 0.18706190\n",
      "Iteration 122, loss = 0.18864515\n",
      "Iteration 123, loss = 0.18766049\n",
      "Iteration 124, loss = 0.18780055\n",
      "Iteration 125, loss = 0.18621863\n",
      "Iteration 126, loss = 0.18754705\n",
      "Iteration 127, loss = 0.18493722\n",
      "Iteration 128, loss = 0.18621253\n",
      "Iteration 129, loss = 0.18685679\n",
      "Iteration 130, loss = 0.18526525\n",
      "Iteration 131, loss = 0.18408028\n",
      "Iteration 132, loss = 0.18366931\n",
      "Iteration 133, loss = 0.18355258\n",
      "Iteration 134, loss = 0.18331187\n",
      "Iteration 135, loss = 0.18296874\n",
      "Iteration 136, loss = 0.18310125\n",
      "Iteration 137, loss = 0.18155902\n",
      "Iteration 138, loss = 0.18083984\n",
      "Iteration 139, loss = 0.18255271\n",
      "Iteration 140, loss = 0.18094060\n",
      "Iteration 141, loss = 0.18074867\n",
      "Iteration 142, loss = 0.18174732\n",
      "Iteration 143, loss = 0.18033837\n",
      "Iteration 144, loss = 0.18006227\n",
      "Iteration 145, loss = 0.18072133\n",
      "Iteration 146, loss = 0.17901869\n",
      "Iteration 147, loss = 0.17889072\n",
      "Iteration 148, loss = 0.17819470\n",
      "Iteration 149, loss = 0.17748742\n",
      "Iteration 150, loss = 0.17778449\n",
      "Iteration 151, loss = 0.17772566\n",
      "Iteration 152, loss = 0.17734352\n",
      "Iteration 153, loss = 0.17954281\n",
      "Iteration 154, loss = 0.17593803\n",
      "Iteration 155, loss = 0.17569174\n",
      "Iteration 156, loss = 0.17570072\n",
      "Iteration 157, loss = 0.17409024\n",
      "Iteration 158, loss = 0.17607491\n",
      "Iteration 159, loss = 0.17530438\n",
      "Iteration 160, loss = 0.17484119\n",
      "Iteration 161, loss = 0.17492545\n",
      "Iteration 162, loss = 0.17601578\n",
      "Iteration 163, loss = 0.17427410\n",
      "Iteration 164, loss = 0.17333917\n",
      "Iteration 165, loss = 0.17348297\n",
      "Iteration 166, loss = 0.17294407\n",
      "Iteration 167, loss = 0.17325332\n",
      "Iteration 168, loss = 0.17383207\n",
      "Iteration 169, loss = 0.17264331\n",
      "Iteration 170, loss = 0.17078804\n",
      "Iteration 171, loss = 0.17118811\n",
      "Iteration 172, loss = 0.17245248\n",
      "Iteration 173, loss = 0.17036195\n",
      "Iteration 174, loss = 0.17052872\n",
      "Iteration 175, loss = 0.17024343\n",
      "Iteration 176, loss = 0.16987655\n",
      "Iteration 177, loss = 0.17083274\n",
      "Iteration 178, loss = 0.17158136\n",
      "Iteration 179, loss = 0.16888044\n",
      "Iteration 180, loss = 0.16895573\n",
      "Iteration 181, loss = 0.16829610\n",
      "Iteration 182, loss = 0.16781179\n",
      "Iteration 183, loss = 0.16949851\n",
      "Iteration 184, loss = 0.16707257\n",
      "Iteration 185, loss = 0.16805465\n",
      "Iteration 186, loss = 0.16787607\n",
      "Iteration 187, loss = 0.16815143\n",
      "Iteration 188, loss = 0.16675543\n",
      "Iteration 189, loss = 0.16571847\n",
      "Iteration 190, loss = 0.16704623\n",
      "Iteration 191, loss = 0.16604756\n",
      "Iteration 192, loss = 0.16595858\n",
      "Iteration 193, loss = 0.16616723\n",
      "Iteration 194, loss = 0.16449357\n",
      "Iteration 195, loss = 0.16436623\n",
      "Iteration 196, loss = 0.16362141\n",
      "Iteration 197, loss = 0.16403869\n",
      "Iteration 198, loss = 0.16365102\n",
      "Iteration 199, loss = 0.16305537\n",
      "Iteration 200, loss = 0.16342639\n",
      "Iteration 201, loss = 0.16508309\n",
      "Iteration 202, loss = 0.16373773\n",
      "Iteration 203, loss = 0.16346619\n",
      "Iteration 204, loss = 0.16199192\n",
      "Iteration 205, loss = 0.16216547\n",
      "Iteration 206, loss = 0.16466035\n",
      "Iteration 207, loss = 0.16203708\n",
      "Iteration 208, loss = 0.16330057\n",
      "Iteration 209, loss = 0.16226347\n",
      "Iteration 210, loss = 0.16319963\n",
      "Iteration 211, loss = 0.16213816\n",
      "Iteration 212, loss = 0.16163165\n",
      "Iteration 213, loss = 0.16085383\n",
      "Iteration 214, loss = 0.16080603\n",
      "Iteration 215, loss = 0.16239482\n",
      "Iteration 216, loss = 0.16039666\n",
      "Iteration 217, loss = 0.15972959\n",
      "Iteration 218, loss = 0.15954895\n",
      "Iteration 219, loss = 0.16062195\n",
      "Iteration 220, loss = 0.16074413\n",
      "Iteration 221, loss = 0.15886702\n",
      "Iteration 222, loss = 0.15736865\n",
      "Iteration 223, loss = 0.15952352\n",
      "Iteration 224, loss = 0.15833197\n",
      "Iteration 225, loss = 0.15939215\n",
      "Iteration 226, loss = 0.15860308\n",
      "Iteration 227, loss = 0.15878589\n",
      "Iteration 228, loss = 0.15871661\n",
      "Iteration 229, loss = 0.15764495\n",
      "Iteration 230, loss = 0.15784998\n",
      "Iteration 231, loss = 0.15856678\n",
      "Iteration 232, loss = 0.15877066\n",
      "Iteration 233, loss = 0.15733422\n",
      "Iteration 234, loss = 0.15580503\n",
      "Iteration 235, loss = 0.15764412\n",
      "Iteration 236, loss = 0.15723121\n",
      "Iteration 237, loss = 0.15823213\n",
      "Iteration 238, loss = 0.15687959\n",
      "Iteration 239, loss = 0.15608456\n",
      "Iteration 240, loss = 0.15552933\n",
      "Iteration 241, loss = 0.15469247\n",
      "Iteration 242, loss = 0.15587392\n",
      "Iteration 243, loss = 0.15581447\n",
      "Iteration 244, loss = 0.15527414\n",
      "Iteration 245, loss = 0.15492705\n",
      "Iteration 246, loss = 0.15475811\n",
      "Iteration 247, loss = 0.15420259\n",
      "Iteration 248, loss = 0.15487884\n",
      "Iteration 249, loss = 0.15479789\n",
      "Iteration 250, loss = 0.15454818\n",
      "Iteration 251, loss = 0.15470145\n",
      "Iteration 252, loss = 0.15253956\n",
      "Iteration 253, loss = 0.15299180\n",
      "Iteration 254, loss = 0.15402068\n",
      "Iteration 255, loss = 0.15521475\n",
      "Iteration 256, loss = 0.15325147\n",
      "Iteration 257, loss = 0.15264902\n",
      "Iteration 258, loss = 0.15228987\n",
      "Iteration 259, loss = 0.15533300\n",
      "Iteration 260, loss = 0.15405256\n",
      "Iteration 261, loss = 0.15275724\n",
      "Iteration 262, loss = 0.15125046\n",
      "Iteration 263, loss = 0.15238857\n",
      "Iteration 264, loss = 0.15143954\n",
      "Iteration 265, loss = 0.15275232\n",
      "Iteration 266, loss = 0.15129406\n",
      "Iteration 267, loss = 0.15066545\n",
      "Iteration 268, loss = 0.15196664\n",
      "Iteration 269, loss = 0.15146223\n",
      "Iteration 270, loss = 0.14906614\n",
      "Iteration 271, loss = 0.15054101\n",
      "Iteration 272, loss = 0.15232735\n",
      "Iteration 273, loss = 0.15243309\n",
      "Iteration 274, loss = 0.15088680\n",
      "Iteration 275, loss = 0.15184031\n",
      "Iteration 276, loss = 0.15165456\n",
      "Iteration 277, loss = 0.14931412\n",
      "Iteration 278, loss = 0.14918833\n",
      "Iteration 279, loss = 0.15068236\n",
      "Iteration 280, loss = 0.14898439\n",
      "Iteration 281, loss = 0.14964414\n",
      "Iteration 282, loss = 0.14889105\n",
      "Iteration 283, loss = 0.14867814\n",
      "Iteration 284, loss = 0.15015061\n",
      "Iteration 285, loss = 0.14871821\n",
      "Iteration 286, loss = 0.14953585\n",
      "Iteration 287, loss = 0.15013463\n",
      "Iteration 288, loss = 0.14925667\n",
      "Iteration 289, loss = 0.14925889\n",
      "Iteration 290, loss = 0.14941481\n",
      "Iteration 291, loss = 0.14767096\n",
      "Iteration 292, loss = 0.14732568\n",
      "Iteration 293, loss = 0.14856831\n",
      "Iteration 294, loss = 0.14719675\n",
      "Iteration 295, loss = 0.14724171\n",
      "Iteration 296, loss = 0.14651928\n",
      "Iteration 297, loss = 0.14687014\n",
      "Iteration 298, loss = 0.14932237\n",
      "Iteration 299, loss = 0.14782584\n",
      "Iteration 300, loss = 0.14739518\n",
      "Iteration 301, loss = 0.14688320\n",
      "Iteration 302, loss = 0.14612919\n",
      "Iteration 303, loss = 0.14491059\n",
      "Iteration 304, loss = 0.14485105\n",
      "Iteration 305, loss = 0.14649709\n",
      "Iteration 306, loss = 0.14431850\n",
      "Iteration 307, loss = 0.14479930\n",
      "Iteration 308, loss = 0.14483252\n",
      "Iteration 309, loss = 0.14435205\n",
      "Iteration 310, loss = 0.14655802\n",
      "Iteration 311, loss = 0.14607652\n",
      "Iteration 312, loss = 0.14726307\n",
      "Iteration 313, loss = 0.14515766\n",
      "Iteration 314, loss = 0.14498606\n",
      "Iteration 315, loss = 0.14459297\n",
      "Iteration 316, loss = 0.14411475\n",
      "Iteration 317, loss = 0.14341649\n",
      "Iteration 318, loss = 0.14413958\n",
      "Iteration 319, loss = 0.14413079\n",
      "Iteration 320, loss = 0.14638158\n",
      "Iteration 321, loss = 0.14367338\n",
      "Iteration 322, loss = 0.14495160\n",
      "Iteration 323, loss = 0.14457485\n",
      "Iteration 324, loss = 0.14332071\n",
      "Iteration 325, loss = 0.14248572\n",
      "Iteration 326, loss = 0.14466781\n",
      "Iteration 327, loss = 0.14530690\n",
      "Iteration 328, loss = 0.14302274\n",
      "Iteration 329, loss = 0.14111778\n",
      "Iteration 330, loss = 0.14201068\n",
      "Iteration 331, loss = 0.14292905\n",
      "Iteration 332, loss = 0.14293772\n",
      "Iteration 333, loss = 0.14165518\n",
      "Iteration 334, loss = 0.14332737\n",
      "Iteration 335, loss = 0.14435284\n",
      "Iteration 336, loss = 0.14370801\n",
      "Iteration 337, loss = 0.14357990\n",
      "Iteration 338, loss = 0.14221814\n",
      "Iteration 339, loss = 0.14196138\n",
      "Iteration 340, loss = 0.14169939\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#108 -> 55 -> 55 -> 1\n",
    "\n",
    "rede_neural_census = MLPClassifier(verbose = True,\n",
    "                                   max_iter= 1000,\n",
    "                                   tol=0.000010,\n",
    "                                   hidden_layer_sizes=(55,55))\n",
    "rede_neural_census.fit(X_census_treinamento, y_census_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' >50K'],\n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes_census = rede_neural_census.predict(X_census_teste)\n",
    "previsoes_census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' <=50K'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_census_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8094165813715456"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy_score(y_census_teste, previsoes_census)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8094165813715456"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "cm=ConfusionMatrix(rede_neural_census)\n",
    "cm.fit(X_census_treinamento, y_census_treinamento)\n",
    "cm.score(X_census_teste, y_census_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.87      0.87      0.87      3693\n",
      "        >50K       0.61      0.61      0.61      1192\n",
      "\n",
      "    accuracy                           0.81      4885\n",
      "   macro avg       0.74      0.74      0.74      4885\n",
      "weighted avg       0.81      0.81      0.81      4885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_census_teste, previsoes_census))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
